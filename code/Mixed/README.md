## Mixed Workloads
For comparing `ThreadPoolExecutor` with `ProcessPoolExecutor` with mixed workloads, the tasks of interest would need to involve both I/O and CPU workloads. As such, the tasks we delegated for the mixed workloads were standard image processing (resizing an image and saving it to the directory) and ML image classification (through the ResNet18 model). These tasks involve both I/O-bound operations, such as loading, reading, and saving images, as well as CPU-bound operations, such as image resizing and model computation. 

Both tasks are done on a set of three images: a high resolution image (1792x1024), a low resolution image (1024x1024), and a white noise image. The high resolution and low resolution images were each randomly chosen from their own respective set of 5 images, while the white noise image was randomly generated. This image selection and generation process ensures some ensemble of randomness and variety in our workload more suitable for a real-world context, rather than performing these tasks with the same set of images every run. To evaluate the efficiency of the two executor classes, we measured time, throughput, CPU usage, and memory usage for each of the tasks with each executor. 


### Files 
- `image_processing.py`: A Jupyter Notebook containing code for opening and resizing the three images. Measures time taken for thread pool execution and process pool execution for this operation.
- `ML_image_processing.py`: A Jupyter Notebook containing code for classifying the three images based on a pretrained ResNet18 model. Measures the time taken for thread pool execution and process pool execution for this operation.
- `image_gen.py`: A file containing the functions for randomly choosing the high/low res images and for generating the random white noise image.
- **high-res-images**: A folder containing five high resolution JPG files (high-res-1.jpg, high-res-2.jpg, etc.) All images were generated by artificial intelligence.
- **low-res-images**: A folder containing five low resolution JPG files (low-res-1.jpg, low-res-2.jpg, etc.) All images were generated by artificial intelligence.

### How to Run
1. Clone this repository, or download this repository via ZIP on your local device.
2. If the repository is contained in a ZIP file, extract the files into a known directory.
3. Open a terminal on your local device (Command prompt, Visual Studio, etc.)
4. Navigate to the location of the repository on your local device.
5. To run each of the files, run this command on your terminal: `python <file-name>.py` where `file-name` is replaced by either "ML_image_processing" or "image_processing" depending on which file you wish to run.

### Results and Limitations
ThreadPoolExecutor performs slightly better than ProcessPoolExecutor for standard image processing in terms of time (0.29 seconds vs. 0.39 seconds) and throughput (9.01 images/sec vs. 7.66 images/sec), likely because this task requires I/O operations like loading and resizing images, meaning that threads can run concurrently without significant overhead. In addition, the CPU and memory usage for both classes are similar and relatively low, which is reasonable given that the image processing task is simple.

For ML image classification, ThreadPoolExecutor appears to be significantly faster than ProcessPoolExecutor. The CPU usage for both executor classes are similar and are higher than the previous task, matching the increased computational complexity of image classification. With regards to performance, it is necessary to mention that the time and throughput measurements for ProcessPoolExecutor are likely significantly higher than they should be. This is likely because the executor loads an instance of the entire model for each image/process. This is also evidenced by the substantially low memory usage rate (0.11 MB), implying that the model is cleared from memory after each process is finished, triggering the necessity to load it repeatedly for each subsequent process. This is redundant and creates a lot of overhead. In general, machine learning models seem to create a multitude of issues for ProcessPoolExecutor. As such, we were not able to find a workaround for this issue in the implementation given our resources; ideally, we would want the model to be in some global, shared memory that each process is able to access. If this can be achieved, it is extremely possible that ProcessPoolExecutor would experience better performance than ThreadPoolExecutor, as it tends to be more favorable for CPU heavy workloads. For now, we can still assert that ThreadPoolExecutor is the better choice for mixed workloads, due to better performance as well as less potential issues.
